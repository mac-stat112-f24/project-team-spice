{
  "hash": "f826b01e7f8c3108e425095433a7fefc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lyrical Analysis\"\nnumber-sections: true\nexecute: \n  warning: false\nfig-env: 'figure'\nfig-pos: 'h'\nfig-align: center\ncode-fold: false\n---\n\n\n\n\nHello\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlyrics <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-14/lyrics.csv')\n\nstudio_album_tracks <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-14/studio_album_tracks.csv')\n\nrelated_artists <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-14/related_artists.csv')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n#For each album, calculate mean values for danceability, energy, and valence\nstudio_album_tracks %>%\n  group_by(album_name) %>%\n  summarise(\n    danceability_mean = mean(danceability),\n    energy_mean = mean(energy),\n    valence_mean = mean(valence)) %>%\n  ungroup() %>%\n#Set factor levels of album_name\n  mutate(\n    album_name = factor(\n      album_name, levels = c(\"Spice\", \"Spiceworld\", \"Forever\"))) %>%\n  arrange(album_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  album_name danceability_mean energy_mean valence_mean\n  <fct>                  <dbl>       <dbl>        <dbl>\n1 Spice                  0.694       0.731        0.756\n2 Spiceworld             0.570       0.776        0.642\n3 Forever                0.695       0.722        0.583\n```\n\n\n:::\n:::\n\n\n\n\nWhat do the variables mean?:\n\nData link: https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-12-14/readme.md\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Who is the most popular member?\nlibrary(tidyverse)\nmember_lines <- lyrics |>\n  select(section_artist) |> \n  mutate(Sporty = str_detect(section_artist, \"Sporty\")) |> \n  mutate(Scary = str_detect(section_artist, \"Scary\")) |>\n  mutate(Posh = str_detect(section_artist, \"Posh\")) |>\n  mutate(Baby = str_detect(section_artist, \"Baby\")) |> \n  mutate(Ginger = str_detect(section_artist, \"Ginger\")) |> \n  summarize(\n    total_Sporty = sum(Sporty, na.rm = TRUE),\n    total_Scary = sum(Scary, na.rm = TRUE),\n    total_Posh = sum(Posh, na.rm = TRUE),\n    total_Baby = sum(Baby, na.rm = TRUE),\n    total_Ginger = sum(Ginger, na.rm = TRUE)\n  ) \n\nmember_lines <- member_lines |> \n  pivot_longer(cols = everything(), \n               names_to = \"Member\", \n               values_to = \"Number of Lines\") |> \n  mutate(Member = str_replace(Member, \"total_\", \"\"))\n\nggplot(member_lines |> \n         mutate(Member = fct_reorder(Member, `Number of Lines`)), \n       aes(x = Member, y = `Number of Lines`, fill = Member)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Number of Total Lines for each Spice Girl\",\n    x = \"Spice Girls\",\n    y = \"Number of Lines\"\n  ) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") \n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nPerhaps different albums promoted different members? Released in 1996, Scary Spice was actually featured the most in their first album, *Spice*. Coming in second and third were Ginger and Baby Spice. The release of the second album *Spiceworld*, however, placed Sporty Spice in a more prominent role musically. She rose from having the fourth most lines to being the clear favorite, as the graph below shows. Scary Spice fell to third, with Baby Spice rising to second and Ginger falling further behind. Why did Sporty Spice receive so many more during the second album? There may not be one clear answer. Audience preferences, musical ability, and more are all factors that could explain the change. One thing is certain, however: Sporty Spice, on paper, became the most popular member of the Spice Girls.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#What About Individual Albums?\nmember_album_lines_per_album <- lyrics |>\n  select(album_name, section_artist) |> \n  mutate(\n    Sporty = str_detect(section_artist, \"Sporty\"),\n    Scary = str_detect(section_artist, \"Scary\"),\n    Posh = str_detect(section_artist, \"Posh\"),\n    Baby = str_detect(section_artist, \"Baby\"),\n    Ginger = str_detect(section_artist, \"Ginger\")\n  ) |> \n  group_by(album_name) |> \n  summarize(\n    total_sporty = sum(Sporty, na.rm = TRUE),\n    total_scary = sum(Scary, na.rm = TRUE),\n    total_posh = sum(Posh, na.rm = TRUE),\n    total_baby = sum(Baby, na.rm = TRUE),\n    total_ginger = sum(Ginger, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |> \n  pivot_longer(\n    cols = starts_with(\"total_\"),\n    names_to = \"Member\",\n    values_to = \"Number of Lines\"\n  ) |> \n  mutate(Member = str_replace(Member, \"total_\", \"\"))  # Clean up the member names\n\n# Visualize the number of lines per member by album\nggplot(member_album_lines_per_album |> \n         mutate(Member = fct_reorder(Member, `Number of Lines`)), \n                aes(x = album_name, y = `Number of Lines`, fill = Member)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Number of Lines for Each Spice Girl by Album\",\n    x = \"Album\",\n    y = \"Number of Lines\"\n  ) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nBut what about the third album? Geri Halliwell-Horton left the group in 1998 to the shock of fans citing creative differences, exhaustion, as well as rumored arguments with Mel B. In response, The four remaining members remained and adapted their musical style and image for their next album *Forever*.\n\nDespite a new R/B sound, Sporty Spice remained as the member with the most lines. Interestingly, the album has the least amount of lines out of their entire discography.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#What are the main lyrical themes? Let's start with top words\nlibrary(tokenizers)\nlibrary(tidytext)\nlyrics_data <- lyrics |> \n  mutate(clean_lyrics = str_to_lower(line),  # Convert to lowercase\n         clean_lyrics = str_replace_all(clean_lyrics, \"[^a-z\\\\s]\", \"\"),  # Remove punctuation\n         clean_lyrics = str_squish(clean_lyrics))  # Remove extra spaces\n\ntokens <- lyrics_data |> \n  unnest_tokens(word, clean_lyrics)  # 'word' column will contain individual words\n\nbigrams <- lyrics_data |> \n  unnest_tokens(bigram, clean_lyrics, token = \"ngrams\", n = 2)\n\ndata(\"stop_words\")\ntokens <- tokens |> \n  anti_join(stop_words, by = \"word\")\n\nword_counts <- tokens |> \n  count(word, sort = TRUE)\n\n# View top words\nhead(word_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  word      n\n  <chr> <int>\n1 love    137\n2 time    105\n3 wanna   102\n4 youre    93\n5 dont     92\n6 la       85\n```\n\n\n:::\n\n```{.r .cell-code}\nword_counts |> \n  slice_max(n, n = 20) |> \n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top Words in Song Lyrics\", x = \"Word\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Sentiment analysis\n\n# Example using Bing lexicon\nsentiment_scores <- tokens |> \n  inner_join(get_sentiments(\"bing\"), by = \"word\") |> \n  count(sentiment, sort = TRUE)\n\n# Visualize sentiment\nsentiment_scores |> \n  ggplot(aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  labs(title = \"Sentiment Analysis of Lyrics\", x = \"Sentiment\", y = \"Count\") +\n  scale_fill_manual(values = c(\"positive\" = \"green\", \"negative\" = \"red\"))\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(tokenizers)\n\ntf_idf <- tokens |> \n  count(song_name, word, sort = TRUE) |>  # Replace 'song_id' with a song identifier column\n  bind_tf_idf(word, song_name, n)\n\n# View top TF-IDF words\ntf_idf |> \n  arrange(desc(tf_idf)) |> \n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 6\n   song_name               word           n    tf   idf tf_idf\n   <chr>                   <chr>      <int> <dbl> <dbl>  <dbl>\n 1 \"Saturday Night Divas\"  deeper        41 0.247  3.43  0.848\n 2 \"If U Can\\x92t Dance\"   dance         60 0.278  2.74  0.761\n 3 \"Holler\"                holler        48 0.194  3.43  0.665\n 4 \"Naked\"                 naked         29 0.190  3.43  0.651\n 5 \"Move Over\"             generation    41 0.237  2.74  0.650\n 6 \"Saturday Night Divas\"  saturday      29 0.175  3.43  0.600\n 7 \"Spice Up Your Life\"    la            64 0.249  2.34  0.582\n 8 \"Mama\"                  loving        22 0.168  2.74  0.460\n 9 \"Weekend Love\"          weekend       16 0.126  3.43  0.433\n10 \"Something Kinda Funny\" kinda         18 0.126  3.43  0.432\n```\n\n\n:::\n\n```{.r .cell-code}\nbigram_counts <- bigrams |> \n  count(bigram, sort = TRUE)\n\n\nlibrary(topicmodels)\n\n# Create a document-term matrix\ndtm <- tokens |> \n  count(song_name, word) |> \n  cast_dtm(song_name, word, n)\n\n# Fit LDA model\nlda_model <- LDA(dtm, k = 3, control = list(seed = 123))  # 'k' is the number of topics\ntopics <- tidy(lda_model, matrix = \"beta\")\n\n# View top words per topic\ntopics |> \n  group_by(topic) |> \n  slice_max(beta, n = 10) |> \n  ungroup() |> \n  ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"Top Words per Topic\", x = \"Word\", y = \"Beta\")\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\ntopics <- tidy(lda_model, matrix = \"beta\")  # \"beta\" represents the word-topic distributions\n\ntop_words <- topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%  # Get top 10 words for each topic\n  ungroup()\n\n# Visualize the top words for each topic\ntop_words %>%\n  ggplot(aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(title = \"Top Words per Topic\", x = \"Word\", y = \"Beta\")\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud)\ntopics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%\n  with(wordcloud(term, beta, max.words = 10, colors = brewer.pal(8, \"Dark2\")))\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n\n```{.r .cell-code}\nSong_topics <- tidy(lda_model, matrix = \"gamma\")  # 'gamma' represents the document-topic distributions\n\nSong_topics_wide <- Song_topics %>%\n  pivot_wider(\n    names_from = topic,  # The topic column will become the new column names\n    values_from = gamma,  # The gamma values will fill the columns\n  )\n\n# View document-topic distribution\nhead(Song_topics_wide)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  document               `1`      `2`       `3`\n  <chr>                <dbl>    <dbl>     <dbl>\n1 2 Become 1       0.186     0.0875   0.727    \n2 Denying          1.00      0.000163 0.000163 \n3 Do It            0.000165  0.000165 1.00     \n4 Get Down With Me 0.999     0.000318 0.000318 \n5 Goodbye          0.000264  0.000264 0.999    \n6 Holler           0.0000949 1.00     0.0000949\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#topic distribution by album\n\ntopics_wide <- read_csv(\"data/topics_wide.csv\")\n\n\nalbum_topics <- topics_wide |> \n  group_by(Album) |> \n  summarize(mean_1 = mean(`1`), mean_2 = mean(`2`), mean_3 = mean(`3`),\n            .groups = \"drop\") |> \n  pivot_longer(cols = starts_with(\"mean\"), names_to = \"topics\", \n               values_to = \"mean of topics\")\n\nggplot(album_topics |> \n          mutate(Album = fct_reorder(Album, `mean of topics`)), \n       aes(x = Album, y = `mean of topics`, fill = topics)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Topic Distribution Across Spice Girls Albums\",\n    x = \"Album\",\n    y = \"Mean Topic Proportion\",\n    fill = \"Topic\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](eda-enrico_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\\#**What is TF-IDF?**\n\nTF-IDF stands for term frequency-inverse document frequency. This model can be applied to a few different purposes, but I use it specifically for text summarization and keyword extraction to find and quantify the importance of specific words in Spice Girls songs.\n\nTerm frequency: There are multiple ways to define frequency, but here, it means the frequency of a particular word in the songs; how often a word is said in a song.\n\nInverse document frequency: I use this to find how common or uncommon a word is in the lyrics. This is particularly important to filter out filler words like \"of\" and \"as.\"\n\nUsing both TF and IDF, I tried to find which words hold the most relevance in their songs. The higher the TF-IDF score, the more relevance the word holds in the song, while the less relevant it is, the closer the score is to zero.\n\nThis model isn't perfect, though. There may be some words, like \"love,\" that are extremely common in their songs. However, their tf-idf score may be low because it is not especially unique, though it is still extremely important.\n\nhttps://www.capitalone.com/tech/machine-learning/understanding-tf-idf/\n\n\\#**What is LDA Analysis?**\n\nLDA analysis is a \"probabilistic generative model\" used here to analyze a collection of text–Spice Girls lyrics. You could also use this for articles, books, and other text. LDA approaches documents as a collection of themes or \"Topics,\" which it them hopes to uncover by analyzing the words of the document.\n\nhttps://medium.com/@pinakdatta/understanding-lda-unveiling-hidden-topics-in-text-data-9bbbd25ae162\n\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#:\\~:text=In%20natural%20language%20processing%2C%20latent,of%20a%20Bayesian%20topic%20model.\n",
    "supporting": [
      "eda-enrico_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}